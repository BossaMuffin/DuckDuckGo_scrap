{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bossa Muffin Scrapping DuckDuckGo Program v3 - 24/04/2022 \n",
    "# With Requests modul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install all package i need for this program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't execute if not need\n",
    "'''\n",
    "import sys\n",
    "!{sys.executable} -m pip install requests\n",
    "!{sys.executable} -m pip install fake_useragent\n",
    "!{sys.executable} -m pip install bs4\n",
    "!{sys.executable} -m pip install time\n",
    "!{sys.executable} -m pip install json\n",
    "!{sys.executable} -m pip install pandas\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all moduls i need for this program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent \n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A class to prepare the research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL_TO_TEST_MY_IP = 'https://httpbin.org/ip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research parameters and arguments \n",
    "class TheResearch:\n",
    "    \n",
    "    def __init__(self, words_list):\n",
    "        self.POST_PARAMS = ['q', 's', 'nextParams', 'v', 'l', 'o', 'json', 'dc', 'api', 'vdq', 'kl' ]\n",
    "        # interesting nodes in the research soup response\n",
    "        self.CLASS_TARGETS = ['result__a', 'result__link', 'result__snippet']\n",
    "        # Evite de faire toujours la même recherche dans DDG pedant les tests -> diminue le risque de blocage serveur \n",
    "        # The payload search request\n",
    "        self.PAYLOAD = {\n",
    "            'q': random.choice(words_list),\n",
    "            'b': '',\n",
    "            'kl': 'fr-fr'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = ['France', 'Président 2022', 'Macron', 'Election', 'Marine Lepen score', 'taux d\\'Abstention', 'élection présidentielle', 'programme Macron', 'rapport GIEC 2022', 'résultat élection']\n",
    "search = TheResearch(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrap_session:\n",
    "    \n",
    "    def __init__(self, payload, url = \"https://html.duckduckgo.com/html\"):\n",
    "        self.URL = url\n",
    "        ## Response dict\n",
    "        self.responses = {}\n",
    "        ## Soup string \n",
    "        self.soup = ''\n",
    "        ## Pages counter\n",
    "        self.page= 0\n",
    "        ## Number of scraped nodes\n",
    "        self.nb_nodes = 1\n",
    "    \n",
    "    # Generate a strong user agent for the session\n",
    "    # Example of a full payload : payload = { 'q': 'test', 's': '29', 'nextParams':'', 'v': 'l', 'o': 'json', 'dc': '32', 'api': 'd.js', 'vqd': 'default', 'kl': 'wt-wt' }\n",
    "    def _defineHeader(self):\n",
    "        ua = UserAgent().random\n",
    "        return {'user-agent': ua}\n",
    "        \n",
    "    # Import the page results from the given URL and datas in post method by payload\n",
    "    def getPage(self, proxy, post_mod=False):\n",
    "        r = False\n",
    "        ipport = proxy['ip']+':' + proxy['port']\n",
    "        header = self._defineHeader()\n",
    "        \n",
    "        if post_mod == False:\n",
    "            try:\n",
    "                self.responses[self.page] = s.get(self.URL, \n",
    "                                                  proxies={'http': ipport , 'https': ipport}, \n",
    "                                                  timeout = proxy['timeout'], \n",
    "                                                  headers = header\n",
    "                                                 )\n",
    "                r=True\n",
    "            except:\n",
    "                print('GET : Failed to join the server via : ', ipport)\n",
    "            '''\n",
    "            print('Status code', self.responses[self.page].status_code)\n",
    "            print('Headers\\' page', slef.responses[self.page].headers['content-type'])\n",
    "            print('Cookies of the session n°', page, 'is :', s.cookies.get_dict())\n",
    "            print('Current payload :', search.payload)\n",
    "            '''\n",
    "        else:\n",
    "            try:\n",
    "                self.responses[self.page] = s.post(self.URL, \n",
    "                                                   proxies={'http': ipport , 'https': ipport}, \n",
    "                                                   timeout=proxy['timeout'], \n",
    "                                                   data=search.PAYLOAD, \n",
    "                                                   cookies=self.responses[self.page-1].cookies, \n",
    "                                                   headers=self.header\n",
    "                                                  ) \n",
    "                r=True\n",
    "            except:\n",
    "                 print('GET : Failed to join the server via proxy : ', ipport)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap = Scrap_session(search.PAYLOAD, \"https://html.duckduckgo.com/html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A class to manage different proxies to use in the scrapping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proxies:\n",
    "    \n",
    "    def __init__(self, proxies_csv = 'proxies_list.csv'):\n",
    "        self._PROXIES_FILE = proxies_csv\n",
    "        self.LIST_TO_USE = self._extractProxiesList(self._PROXIES_FILE)\n",
    "    \n",
    "\n",
    "    # Transform string milisecond '1389 ms' in an int second '1.5'\n",
    "    def _strMsToIntS(self, str_ms):\n",
    "        int_ms=int(str_ms.replace(' ms', ''))\n",
    "        #print(int_ms)\n",
    "        s=(int_ms//1000)*1.5\n",
    "        #print(s)\n",
    "        return int_ms\n",
    "    \n",
    "    def _extractProxiesList(self, file):\n",
    "        proxies_list = []\n",
    "        with open(file, 'r') as f:\n",
    "            reader_f = csv.reader(f)\n",
    "            proxies_list = [{'ip':row[0], 'port':row[1], 'timeout':self._strMsToIntS(row[3]), 'prot':row[2], 'last_status':0} for row in reader_f]\n",
    "        return proxies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = Proxies('proxies_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example result of the miliseconds string to seconds int\n",
    "proxies._strMsToIntS('100 ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatResults(soup):\n",
    "    # Select the payload tab prepare to post method\n",
    "    nodes_payload={}\n",
    "\n",
    "    for param in search.POST_PARAMS:\n",
    "        nodes_payload[param] = scrap.soup.find_all(attrs={\"name\": param})\n",
    "        if len(nodes_payload[param]) != 0:\n",
    "            print('Taille!=0 : ', len(nodes_payload[param]))\n",
    "            print('POP : ', nodes_payload[param].pop())\n",
    "            search.PAYLOAD[param] = nodes_payload[param].pop()['value']\n",
    "        else:\n",
    "            search.PAYLOAD[param] = ''\n",
    "\n",
    "    # Select the interesting object\n",
    "    nodes = {}\n",
    "    for target in search.CLASS_TARGETS:\n",
    "        nodes[target] = scrap.soup.find_all(class_=target)\n",
    "\n",
    "    nb_nodes = len(nodes[class_targets[0]])\n",
    "    print('nb results', nb_nodes)\n",
    "    \n",
    "    ## ------------------------------------------------\n",
    "    ## Format intersting elements in a shaped tab / bdd\n",
    "    ## ------------------------------------------------\n",
    "    tab_results = []\n",
    "    \n",
    "    for i in range(nb_nodes):\n",
    "        result = {}\n",
    "        result['title'] = nodes_titles[i].text\n",
    "        result['link'] = nodes_links[i].text\n",
    "        result['text'] = nodes_texts[i].text\n",
    "        tab_results.append(result)\n",
    "        \n",
    "    return tab_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResultsInCsvFile(tab_results : dict) -> str:\n",
    "    bdd_results = pd.DataFrame(tab_results, columns=['title', 'link', 'text'])\n",
    "\n",
    "    print('Number of elements : ', scrap.nb_nodes)\n",
    "    print('\\n')\n",
    "    print(bdd_results)\n",
    "    \n",
    "    ## ------------------------------------------------\n",
    "    ## Write the tab in a CSV file\n",
    "    ## ------------------------------------------------\n",
    "    clean_word_request = removeSpecialChars(payload['q'])\n",
    "    csv_name_file = 'duckduckgo_requests_results_' + clean_word_request + '.csv'\n",
    "    bdd_results.to_csv(csv_name_file)\n",
    "\n",
    "    return csv_name_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialChars(payload_word_request: str) -> str:\n",
    "    payload_word_request = re.sub('[^a-zA-Z0-9 \\n\\.]', '_', payload_word_request)\n",
    "    return payload_word_request.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(removeSpecialChars('hé_ )°ho'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a virgin session with the website on the landing html page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapResultsPage():\n",
    "    n=0\n",
    "    while scrap.nb_nodes != 0 and scrap.page < 10 and n < len(proxies.LIST_TO_USE) :\n",
    "        ## Don't flood the server !!!!\n",
    "        time.sleep(3)\n",
    "        print('\\n')\n",
    "        print('Ready to scrap the page n°', scrap.page)\n",
    "\n",
    "        # We test the proxies_list until find a working proxy to do the request on th target page\n",
    "        while n < len(proxies.LIST_TO_USE) and proxies.LIST_TO_USE[n]['last_status'] != 200 :\n",
    "            time.sleep(0.1)\n",
    "            print('\\n')\n",
    "            print('Proxy n°', n, ' -> ', proxies.LIST_TO_USE[n]['ip'],':', proxies.LIST_TO_USE[n]['port'])\n",
    "            ## Import the page results from the given URL \n",
    "            ## datas in payload (as search request) are in the post method\n",
    "            request_success = scrap.getPage(proxies.LIST_TO_USE[n], post_mod=True)\n",
    "            \n",
    "            try:\n",
    "                proxies.LIST_TO_USE[n]['last_status'] = scrap.responses[scrap.page].status_code\n",
    "                \n",
    "            except KeyError:\n",
    "                request_success = False\n",
    "                proxies.LIST_TO_USE[n]['last_status'] = 0\n",
    "                print('>\tPROXY ERROR')\n",
    "                print('>\tResults page is unscrapable with this script')\n",
    "                print('>\tDefine status code to 0 on page n°', scrap.page)\n",
    "                # Change proxy if current proxy fail\n",
    "                n+=1\n",
    "\n",
    "            if request_success and proxies.LIST_TO_USE[n]['last_status'] == 200:\n",
    "                # The proxy n works !\n",
    "                # The imported code is put in a soup object, easy to work on\n",
    "                scrap.soup = BeautifulSoup(scrap.responses[scrap.page].text, \"html.parser\")\n",
    "                dict_results = formatResults(scrap.soup)\n",
    "                saveResultsInCsvFile(dict_results)\n",
    "                ## Now, we scrap a new page\n",
    "                scrap.page += 1\n",
    "\n",
    "            elif request_success and proxies.LIST_TO_USE[n]['last_status'] != 200:\n",
    "                print('>\tREQUEST ERROR')\n",
    "                print('>\tResults page is unscrapable with this script')\n",
    "                print('>\tStatus code :', proxies.LIST_TO_USE[n]['last_status'], ' on page n°', scrap.page)\n",
    "                # Change proxy if current proxy fail\n",
    "                n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay to pass th 403 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.Session() as s:\n",
    "    scrapResultsPage()\n",
    "print('\\n')\n",
    "print(f\"End of Requests scrapping on : {scrap.URL} with search : '{search.PAYLOAD['q']}'\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit('End') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
